---
title: "Call Of Duty Data Analysis"
author: "Andrew Eross, Owen Wassel, Jack Messina, Khang Le"
date: "2025-12-02"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```



## 1 Introduction

Call of Duty is one of the most popular first person shooter games of all time. As a result, players from all over the world continue to play and try to improve their abilities to win more and more games. Today, we will be investigating the statistics of two Call of Duty Players, Player1 and Player2. We will also be diving into data from the different game modes of Call of Duty: Domination, Kill Confirmed, Hardpoint, and TDM. One of our goals is to learn more about what this data means, and most importantly want to answer some research objectives.

The objectives are as follows:

1. Which game mode is likely to reach the score limit?

2. What variables are predictors of TotalXP?

3. Predictive Match Outcome (Win/Loss)

In this project, we will be applying various statistical methods to answer these questions/objectives with an emphasis on the predictive modeling techniques kNN, Random Forest, and XGBoost. Before we get into those more advanced techniques, we must first explore the data and do necessary wrangling/tidying in order to apply the methods we listed. Objectives 1 and 2 will serve as exploratory data analysis (EDA) objectives to get a solid understanding of the data we are working with to ensure a correct execution of kNN, Random Forest, and XGBoost. 


## 2 Data Summary and Exploratory Data Analysis

### 2.1 Variable Description

There are a total of 3 datasets that we have and will be working with: Player1, Player2, and GameModes. First, let's explore the Player1 and Player2 datasets.

Player1 and Player2 data are structured in a similar fashion and contain the same variables so it will be eaisest to observe them at the same time. The columns of these datasets represent match statistics and the rows represent each match the player participated in. These datasets contain the same amount of columns and statistic types, however Player1 has more rows meaning that they played more games than Player2. Each dataset contains 27 columns which is a lot of variables to work with, however a lot of them contain mostly null values so we will only be using a select amount of them. Those variables are: Choice, Result, Eliminations, Deaths, Score, Damage, and TotalXP.

Now for the GameModes dataset, only 3 variables are listed: Mode, ScoreLimit, and TimeLimit. Obviously these variables tell us the score and time limit of each game mode. These variables will also be useful to explore each match with the score and time limit for reason of the match concluding.

An important value that sticks out in the GameMode dataset is that the game mode Domination has no time limit. This will be worth noting when answer our EDA question below.

### 2.2 Exploratory Data Analysis

We can now perform some exploratory data analysis (EDA) to find relationships with the Player data and game modes data. We will look into which game mode is likely to reach the score limit and what variables are predictors of TotalXP.

 

To discover which game mode is likely to reach the score limit, some data wrangling must be in order. We first combine the Player1 and Player2 data and add a PlayerID column to the merged dataset. Then, we clean the GameModes set by taking off the game modes that contain HC and combining them with their respective game types. For example, the HC - Kill Confirmed game type will be combined with Kill Confirmed. We also removed rows with NA values. Figure 1 below displays the count of number of matches for each game mode.TDM (Team Deathmatch) has by far the most number of matches at 474, and Domination the least at 14.

```{r label = chart1, echo = FALSE, fig.cap = "Count of each Game Mode", fig.height = 4}
remove(list = ls())
library(tidyverse)
library(caret)
library(pROC)
library(ggplot2)
library(dplyr)
library(readxl)
library(lubridate)
library(FNN)
library(xgboost)
library(kableExtra)
library(randomForest)
CODGameModes <- read.csv("~/Documents/GitHub/stat380finalproject/CODGameModes.csv")
CODGames1 <- read.csv("~/Documents/GitHub/stat380finalproject/CODGames_p1_380.csv")
CODGames2 <- read.csv("~/Documents/GitHub/stat380finalproject/CODGames_p2_380.csv")

# Add a Player ID and stack both datasets

games_all <- bind_rows(
CODGames1 %>% mutate(Player = "Player1"),
CODGames2 %>% mutate(Player = "Player2")
)
# Look at raw GameType values
# Remove "HC -", "HC –", etc. and map hardcore versions to base mode

games_all <- games_all %>%
mutate(
GameType_clean = stringr::str_replace(
GameType,
"HC[[:space:]]*[-–][[:space:]]*",
""
)
)
games_split <- games_all %>%
  # Join with game mode metadata
  left_join(CODGameModes, by = c("GameType_clean" = "Mode")) %>%
  
  # Split Result into team & opponent scores
  separate(Result, into = c("TeamScore", "OpponentScore"), 
           sep = "-", remove = FALSE) %>%
  filter(FullPartial == "Full") %>%
  mutate(TeamScore = as.numeric(TeamScore),
         OpponentScore = as.numeric(OpponentScore),
         Result = case_when(
           TeamScore == OpponentScore ~ 0,
           TeamScore > OpponentScore ~ 1,
           TeamScore < OpponentScore ~ 0,
         )) %>%
  filter(!is.na(Result))

# Create a logical/0-1 variable for reaching the score limit
games_split <- games_split %>%
  mutate(
    ReachedLimit = ifelse(TeamScore == ScoreLimit | OpponentScore == ScoreLimit , 1, 0)
  )
limit_summary <- games_split %>%
  group_by(GameType_clean) %>%
  summarise(
    TotalMatches = n(),
    ScoreReached = sum(ReachedLimit, na.rm = TRUE),
    ProportionScoreReached = ScoreReached / TotalMatches
  ) %>%
  rename(GameType = GameType_clean) %>%
  arrange(desc(ProportionScoreReached))


ggplot(limit_summary,
aes(x = reorder(GameType, TotalMatches),
y = TotalMatches, fill = GameType)) +
geom_text(aes(label = TotalMatches), hjust = -0.1) +
geom_col() +
ylim(0,550) +
coord_flip() +
labs(
title = "Count of Matches by Game Mode",
x = "Game Mode",
y = "Number of Matches"
) +
theme_minimal()
```



Next, we perform a join with the Game Modes data with the score and time limit variables. We do this because we want to create a score limit indicator and compare the match score. If these are equal, we yield a 1. Otherwise, 0. We can then calculate the proportions and display the results. The proportion of matches reaching score limit by game mode is shown in Figure 2 below.



```{r  label = chart2, echo = FALSE, fig.cap = "Proportion of Matches Reaching Score Limit by Game Mode", fig.height = 4}
ggplot(limit_summary,
aes(x = reorder(GameType, ProportionScoreReached),
y = ProportionScoreReached, fill = GameType)) +
geom_text(aes(label = round(ProportionScoreReached, 2)), hjust = -0.1) +
geom_col() +
ylim(0,1) +
coord_flip() +
labs(
title = "Proportion of Matches Reaching Score Limit by Game Mode",
x = "Game Mode",
y = "Proportion Reaching Score Limit"
) +
theme_minimal()
```

Based on this visual, we can infer that Domination matches are most likely to reach the score limit since every one of their matches have reached the score limit. This of course makes sense because we pointed out in section 2.1 that Domination matches have no time limit, so matches have to end via score. 

Team Deathmatch (TDM) matches are most likely to end via time limit. However, comparing this percentage with Hardpoint and Kill Confirmed is a bit difficult considering there are so many more TDM matches. If a similar number of matches were played between TDM, Hardpoint, and Kill Confirmed, the comparison in percentage of games ended via score limit might look a bit different.

### 3 Inference Modeling

To accurately identify what variables are predictors of TotalXP, we must once again prepare our data. First, we mutate the TotalXP variable to ScaledXP. The column "XPType" for the Player1 and Player2 datasets indicate the factor a player's raw XP is changed by resulting in their TotalXP. For example, matches are either "10% Boost" or "Double XP + 10% Boost." For 10% boosted matches, the raw XP (what the player actually earned) was multipled by 1.1 to calculate the player's TotalXP. Double XP + 10% Boost matches had raw XP multiplied by 2.1 to find TotalXP. We will calculate our variable ScaledXP by performing the respective inverse operations on TotalXP, so identical game stats for 10% boosted matches and double + boosted matches earn the same XP for the sake of our models all else equal.

After scaling XP, we remove partial games and select only numeric variables from combined Player1 and Player2 dataset which removes map choice, primary weapon, game mode. Next, we remove variables with 50% or more missing values (columns specific to a game mode such as Confirms and Denies). Finally we must see that variables with near-zero variance are removed, but for this case, did not remove any columns. Now that we prepared the data for the model, we can create the model that decides which variables best predict TotalXP.

Our approach to identifying the best predictors involves using an AIC-based stepwise selection process. AIC is goodness-of-fit plus penalty for complexity. This algorithm valuates models by adding/removing predictors and chooses the combination with the lowest AIC, balancing predictive accuracy and model simplicity. Lower AIC values indicate a better model. Each additional predictor increases the penalty, so only variables that significantly improve fit (reduce deviance) are kept. We decided on this process because it provides an objective, numeric criterion for model comparison. For this process, each candidate model’s AIC is compared; a drop of 2+ points in AIC generally indicates a meaningfully better model, while increases signal overfitting. When all was said and done, we obtained a final model containing only the most important predictors of ScaledXP.

The final variables we selected all yielded p-values less than 0.05, which we labeled as statistically significant:

  - **TeamScore**
  - **Eliminations**
  - **Deaths**
  - **Score**
  - **Damage**
  - **ReachedLimit**

Let's look deeper into the Eliminations variable as a means of predicting TotalXP. All else equal, for an increase of one elimination, we expect on average for ScaledXP to increase by about 109.01 (~119.911 increase in TotalXP for 10% XP Boosted matches and ~228.921 increase in TotalXP for Double XP + 10% Boosted matches).

## 4 Machine Learning Methods

Before diving into our predictions for game outcome (win/loss), we must first establish some preliminary steps for a clean machine learning enviornment. The general process for each model (kNN, Random Forest, and XGBoost) is as follows:

  - **Include only full matches**
  - **Consider Draws as Losses**
    - Indicator Variable as 1 for Wins, 0 for losses and draws
    - Focus on winning percentage
  - **Train/Test split of 80% to 20%**
  - **Set Random Seed of 1103**
  - **Set threshold for confusion matrix as 0.555**
    - Equal to the true winning percentage of all matches between players
    
The variables included in each model were Eliminations, Deaths, and Damage. We decided not to include TotalXP or Score in this model, because our intuition thought that these variables would be affected by winning or losing, meaning that winning increases score or XP, so it is not appropriate to consider these potential confounding variables.


### 4.1 k-Nearest Neighbors (kNN)

kNN was used to classify and predict wins and losses based on the variables we decided on (Eliminations, Deaths, and Damage). We created a nearest neighbors model from k values of 1 through 40. We iterated through each value of nearest neighbors and chose the one that yielded the lowest RMSE value. In the end, indicated by Figure 3, k=8 yielded the lowest RMSE at 0.4836806.

```{r, label = chart3, echo = FALSE, fig.cap = "RMSEs by k Nearest Neighbors", fig.height = 4}
winning_pct <- mean(games_split$Result)

set.seed(1103)
inds <- sample(1:nrow(games_split), floor(.8*nrow(games_split)))
set.seed(NULL)

Train <- games_split[inds, ]
Test <- games_split[-inds, ]

xvars <- c("Eliminations", "Deaths", "Damage") 
          
#Initialize
maxK <- 40
mse_vec <- rep(NA, maxK)
rmse_vec <- rep(NA, maxK)

#Loop
for(i in 2:maxK) {
  #Build Model
  knn_res <- knn.reg(train = Train[ , xvars, drop = FALSE],
                   test = Test[ , xvars, drop = FALSE],
                   y = Train$Result,
                   k = i)
  
  #Find MSE
  mse_vec[i] <- mean((Test$Result - knn_res$pred)^2)
  
  #Find RMSE
  rmse_vec[i] <- sqrt(mse_vec[i])
}


#Create storage data frame so we can use ggplot
temp_df <- data.frame(k = 1:maxK, mse = mse_vec, rmse = rmse_vec)

#Create plot
ggplot(data = temp_df, mapping = aes(x = k, y = rmse)) +
  geom_line() +
  theme_minimal()
```
We can now create our final kNN model using k = 8 to predict the result of testing data to the threshold. As a reminder, predicting testing data will return a value on a scale of 0 to 1, and the true winning percentage of the matches is about 55%. Therefore, predicted results above 0.55 will be considered a win (equivalent to 1 in matrix), and anything below will be considered a loss (equivalent to 0 in matrix). For the confusion matrix below (Table 1- Actual results as columns and Predicted as Rows) we will also calculate the accuracy, precision, specificity, and sensitivity metrics in Table 2.


```{r, echo = FALSE}
knn_res8 <- knn.reg(train = Train[ , xvars, drop = FALSE],
                   test = Test[ , xvars, drop = FALSE],
                   y = Train$Result,
                   k = 8)

pred_prob <- knn_res8$pred

#Establish threshold
threshold <- 0.555

pred_win <- ifelse(pred_prob > threshold, 1, 0)

# Create confusion matrix
conf_mat <- table(Predicted = pred_win, Actual = Test$Result)
kable(conf_mat, caption = "Confusion Matrix for kNN Model")
```
```{r, echo = FALSE}
accuracy = (42+50) / (42+50+34+20)
precision = 50 / (50 + 20)
sens <- 50 / (50 + 34)
spec <- 42 / (42 + 20)

var_col <- c("Accuracy", "Precision", "Sensitivity", "Specificity")
val_col <- c(accuracy, precision, sens, spec)

metric_df <- data.frame(
  Metric = var_col,
  Value = val_col
)

kable(metric_df, digits = 3, caption = "Metric Table for kNN Model")
```

The kNN model produces an accuracy of about 63%. This certainly is not great, but better than flipping a coin to predict wins and losses. The model is much better at predicting losses than wins, evident in only 20 False Positives (out of 62 total losses) to 34 False Negatives (out of 84 total wins). A potential conclusion we can draw from this is that a poor player performance is more likely to cause a team loss than a good player performance causing a team win, however more information will be needed to support this claim.

Another metric computed for evaluating the model was the Area Under the Curve (AUC) for the model's Receiver Operating Characteristics (ROC) Curve, which connects True Positive Rate and True Negative Rate pairings at each threshold from 0 to 1. The line y = x is plotted to show a curve with an AUC of 0.50, which would represent a 50/50 prediction of win or loss. For this curve, our AUC was 0.659.


```{r, label = chart4, echo = FALSE, fig.cap = "ROC Curve for kNN", fig.height = 4}
thresholds <- seq(1, 0, -0.01)
TPR_vec <- rep(NA, length(thresholds))
FPR_vec <- rep(NA, length(thresholds))

for(i in 0:length(thresholds)){
  
  t = thresholds[i]
  
  #find values in conf matrix
  TP <- sum(pred_prob > t & Test$Result == 1)
  FN <- sum(pred_prob < t & Test$Result == 1)
  TN <- sum(pred_prob < t & Test$Result == 0)
  FP <- sum(pred_prob > t & Test$Result == 0)

  #Find sensitivity
  TPR_vec[i] <- TP / (TP + FN)
  
  #Find 1-specificity
  FPR_vec[i] <- 1-(TN / (TN + FP))
}

roc_df <- data.frame(threshold = thresholds, TPR = TPR_vec, FPR = FPR_vec)

# Calculate AUC using trapezoidal rule
auc <- sum(diff(roc_df$FPR) * (head(roc_df$TPR, -1) + tail(roc_df$TPR, -1)) / 2)

ggplot(data = roc_df, mapping = aes(x = FPR, y = TPR)) +
  geom_line(color = "black") +
  labs(x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)", title = "Receiver Operating Characteristic (ROC) Curve") +
  geom_abline(intercept=0,slope=1, color = "grey") +
   annotate("text",
           x = 0.6, y = 0.4,           # adjust position if needed
           label = paste("AUC =", round(auc, 3)),
           size = 5) +
  theme_minimal()

roc_df <- roc_df[order(roc_df$FPR), ]
```

### 4.2 Random Forest
Next, Random Forest was used to predict wins based on the same predictors (Eliminations, Deaths, and Damage). For this model, we created a single decision tree using the same random seed as before. Only a single decision tree was made because there were no methods used in the other two models to account for potential overfitting, such as k-fold cross validation. Before that model can be made, however, we must convert the Result variable into a factor with two levels. This ensures that the randomForest function creates a classification tree, rather than a regression tree. 


```{r, echo = FALSE}
set.seed(1103)

TrainRF <- Train %>% mutate(Result = as.factor(Result))
TestRF <- Test %>% mutate(Result = as.factor(Result))

rf_model <- randomForest(Result ~ Eliminations + Deaths + Damage, data = TrainRF)

rf_preds <- predict(rf_model, TestRF)

pred_probs <- predict(rf_model, TestRF, type = 'prob')[,1]

conf_mat <- table(Predicted = rf_preds, Actual = TestRF$Result)
kable(conf_mat, caption = "Confusion Matrix for Random Forest Model")
```

```{r, echo = FALSE}
accuracy = (33+65) / (33+19+29+65)
precision = 65 / (65 + 29)
sens <- 65 / (65 + 19)
spec <- 33 / (33 + 29)

var_col <- c("Accuracy", "Precision", "Sensitivity", "Specificity")
val_col <- c(accuracy, precision, sens, spec)

metric_df <- data.frame(
  Metric = var_col,
  Value = val_col
)

kable(metric_df, digits = 3, caption = "Metric Table for Random Forest Model")
```
The random forest model produced an accuracy of 67.1%, which is slightly better than the kNN model. This model correctly predicts whether the player won or lost approximately 2/3 of the time. It was very effective at correctly identifying the games in which the player won (hence the high precision), however, it failed to identify true negatives, with a specificity (or negative predictive value) of 53.2%, essentially a coin flip.

To futher look into this model, we will now evaluate the AUC of the ROC curve, just like we did for kNN. For this model, the AUC is 0.700, which is slightly better than kNN.

```{r, label = chart5, echo = FALSE, fig.cap = "ROC Curve for Random Forest", fig.height = 4, fig.pos='H'}
thresholds <- seq(1, 0, -0.01)
TPR_vec <- rep(NA, length(thresholds))
FPR_vec <- rep(NA, length(thresholds))

pred_prob <- 1 - predict(rf_model, TestRF, type = 'prob')[,1]

for(i in 0:length(thresholds)){
  
  t = thresholds[i]
  
  #find values in conf matrix
  TP <- sum(pred_prob > t & TestRF$Result == 1)
  FN <- sum(pred_prob < t & TestRF$Result == 1)
  TN <- sum(pred_prob < t & TestRF$Result == 0)
  FP <- sum(pred_prob > t & TestRF$Result == 0)

  #Find sensitivity
  TPR_vec[i] <- TP / (TP + FN)
  
  #Find 1-specificity
  FPR_vec[i] <- 1-(TN / (TN + FP))
}

roc_df <- data.frame(threshold = thresholds, TPR = TPR_vec, FPR = FPR_vec)

# Calculate AUC using trapezoidal rule
auc <- sum(diff(roc_df$FPR) * (head(roc_df$TPR, -1) + tail(roc_df$TPR, -1)) / 2)

ggplot(data = roc_df, mapping = aes(x = FPR, y = TPR)) +
  geom_line(color = "black") +
  labs(x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)", title = "Receiver Operating Characteristic (ROC) Curve") +
  geom_abline(intercept=0,slope=1, color = "grey") +
   annotate("text",
           x = 0.6, y = 0.4,           # adjust position if needed
           label = paste("AUC =", round(auc, 3)),
           size = 5) +
  theme_minimal()

roc_df <- roc_df[order(roc_df$FPR), ]
```


### 4.3 XGBoost
XGBoost (Extreme Gradient Boosting) is a boosting-based machine learning method that builds an ensemble of decision trees sequentially, where each new tree is trained to correct the errors of the previous ones. Because boosting can capture nonlinear relationships and complex interactions between predictors, we applied this method to classify match outcome (win vs. loss) using the same variables as the previous models: Eliminations, Deaths, and Damage.
Before fitting the model, we converted the binary outcome into numeric form (1 = Win, 0 = Loss). We then created model matrices so that XGBoost could work with the numeric predictor set. The training/testing split and threshold setup remained consistent with kNN and Random Forest:
Train/Test split = 80/20
Random Seed = 1103
Threshold = 0.555, equal to the true winning percentage
Values above 0.555 were classified as wins, values below as losses
We trained a binary logistic XGBoost model with 200 boosting rounds, allowing it to iteratively refine predictions by reducing classification error at each stage.
Below is the confusion matrix for the XGBoost model, using the same threshold approach as the other methods:

```{r, label=chart6,echo = FALSE,fig.cap = "ROC Curve for XGBoost", fig.height = 4}
library(xgboost)
library(pROC)
library(caret)
CODGames <- bind_rows(
  CODGames1 %>% mutate(Player = "Player1"),
  CODGames2 %>% mutate(Player = "Player2")
)

CODGamesResults <- CODGames %>% 
  extract(Result, into = c('TeamScore', 'OppScore'), regex = '([0-9]+)-([0-9]+)') %>%
  mutate(
    TeamScore = as.numeric(TeamScore),
    OppScore  = as.numeric(OppScore),
    Outcome   = as.factor(ifelse(TeamScore > OppScore, "Win", "Loss"))
  ) %>%
  select(Eliminations, Deaths, Damage, Outcome) %>%
  drop_na()

set.seed(1103)
inds <- sample(1:nrow(CODGamesResults), floor(.8*nrow(CODGamesResults)))
Train <- CODGamesResults[inds, ]
Test  <- CODGamesResults[-inds, ]

# Convert outcome to numeric
# Correct encoding:
Train$Outcome_num <- ifelse(Train$Outcome == "Win", 1, 0)
Test$Outcome_num  <- ifelse(Test$Outcome == "Win", 1, 0)

# Matrices
train_matrix <- model.matrix(Outcome_num ~ . - Outcome - 1, data = Train)
test_matrix  <- model.matrix(Outcome_num ~ . - Outcome - 1, data = Test)

train_y <- Train$Outcome_num
test_y  <- Test$Outcome_num

# Train model
xgb_model <- xgboost(
  data = train_matrix,
  label = train_y,
  nrounds = 200,
  objective = "binary:logistic",
  verbose = 0
)

# Predict p(win)
xgb_prob <- predict(xgb_model, test_matrix)

# Apply threshold
threshold <- 0.555
xgb_class <- ifelse(xgb_prob > threshold, 1, 0)

# Factors
xgb_class_fac <- factor(ifelse(xgb_class == 1, "Win", "Loss"))
test_outcome_fac <- factor(ifelse(test_y == 1, "Win", "Loss"))

# Confusion Matrix
cm <- confusionMatrix(xgb_class_fac, test_outcome_fac)
kable(cm$table, caption = "Corrected Confusion Matrix for XGBoost")

# Metrics
Accuracy    <- sum(diag(cm$table)) / sum(cm$table)
Precision   <- cm$table["Win","Win"] / (cm$table["Win","Win"] + cm$table["Win","Loss"])
Sensitivity <- cm$table["Win","Win"] / (cm$table["Win","Win"] + cm$table["Loss","Win"])
Specificity <- cm$table["Loss","Loss"] / (cm$table["Loss","Loss"] + cm$table["Loss","Win"])

metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Sensitivity", "Specificity"),
  Value  = round(c(Accuracy, Precision, Sensitivity, Specificity), 4)
)
kable(metrics, caption = "Corrected XGBoost Evaluation Metrics")

# ROC / AUC
xgb_roc <- roc(test_y, xgb_prob)
plot(xgb_roc, main="Corrected ROC Curve"); abline(0,1)
text(0.6, 0.2, paste("AUC =", round(auc(xgb_roc), 3)))


```

The XGBoost model achieved an accuracy of 62.96%, indicating that it correctly predicted match outcomes slightly better than chance. Both precision (61.45%) and specificity (61.45%) suggest that the model performs similarly when predicting losses, correctly identifying negative outcomes at a moderate rate. Meanwhile, the sensitivity (64.56%) shows that the model is somewhat better at identifying wins than losses and AUC score at 0.663. Overall, XGBoost provides a balanced but modest level of predictive performance, capturing some meaningful patterns in player statistics but not reaching the higher accuracy and sensitivity achieved by the Random Forest model.A likely reason XGBoost underperformed is the low dimensionality of the feature set (only Eliminations, Deaths, Damage), which limits the algorithm’s ability to capture complex interactions.

## 5 Conclusion

In this project, we ran 3 different machine learning methods with differing results. Random Forest performed the best in terms of predicting match outcome (win/loss) and XGBoost performed the worst. Random Forest yielded an accuracy of 67.12% which is far better than simply a 50/50 chance at winning a match. Specificity was not as high however with a result of 53.23%. What really stood out in Random Forest was the sensitivity which yielded a 77.38%. This shows that the model does a really good job (relatively speaking) in identifying positive cases, in this case wins. The next best performing model was kNN. The accuracy was a little lower than the Random Forest at 63.014%, but performed better in terms of precision and specificity at 71.43% and 67.74% respectively. Sensitivity was far behind Random Forest's 77.38% though with a 59.52%. Finally, XGBoost was the worst performing model in terms of accuracy across all 3 with a 62.96%. However, specificity was higher than Random Forest at 64.56%. However, with relatively high precision across the rest of the models, XGBoost had the lowest at 61.45%.

```{r, label = chart7, echo = FALSE, fig.cap = "Performance Metrics for Different Methods"}
library(knitr)

results <- data.frame(
  Method = c("Random Forest", "kNN", "XGBoost"),
  Accuracy = c(0.6712, 0.63014, 0.6296),
  Precision = c(0.6915, 0.7143, 0.6145),
  Sensitivity = c(0.7738, 0.5952, 0.6456),
  Specificity = c(0.5323, 0.6774, 0.6456)
)

kable(results, digits = 4, align = "lcccc",
      col.names = c("Method", "Accuracy", "Precision", "Sensitivity", "Specificity"))
```

## 6 AI Statement

All group mates attest to utilizing generative AI (ChatGPT) to assist work throughout the project. AI usage was limited to debugging and not to create entire chunks of code. Each code chunk was created entirely by group members, but certain lines of code were fed into ChatGPT in case code was not outputting as expected, and then the code was modified with AI suggestions. Gen AI was used to assist group work and not to replace it.
